dadaRsSense <- dada(derepRs, err=errR, multithread=TRUE,pool = TRUE)
# We can now merge the seqs
mergers <- mergePairs(dadaFsSense, derepFs, dadaRsSense, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Now we can construct a sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=7, verbose=TRUE)
dim(seqtab.nochim)
#Lets count up the reads for the sense section of the pipeline
#How many reads do we lose?
getN <- function(x) sum(getUniques(x))
track <- cbind(sapply(dadaFsSense, getN), sapply(dadaRsSense, getN), sapply(mergers, getN),rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- paste0("S.",c("denoisedF", "denoisedR", "merged", "lengthtrunc", "nonchim"))
rownames(track) <- names(dadaFsSense)
head(track)
#Let's save the Sense dataset and rerun the pipeline for the Antisense
EUK.Sense <- seqtab.nochim
EUK.SenseTrackedReads <- track
#### 3.2 EUK.ANTISENSE ####
path <- "A.stripped"
fnFsASense <- sort(list.files(path, pattern="EUK.*R1.fastq.gz", full.names = TRUE))
fnRsASense <- sort(list.files(path, pattern="EUK.*R2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- gsub(".A.R1.fastq.gz","",basename(fnFsASense))
#Lets look at the quality for forwards
pdf("../7.DADA2/A_EUK_forwards.pdf")
plotQualityProfile(fnFsASense[94:98])
dev.off()
#...and reverses
pdf("../7.DADA2/A_EUK_reverse.pdf")
plotQualityProfile(fnRsASense[94:98])
dev.off()
# Place filtered files in filtered/ subdirectory
filtFsASense <- file.path("../7.DADA2/filtered", paste0(sample.names, "_A_F_filt.fastq.gz"))
filtRsASense <- file.path("../7.DADA2/filtered", paste0(sample.names, "_A_R_filt.fastq.gz"))
# Filter expression
out <- filterAndTrim(fnFsASense, filtFsASense, fnRsASense, filtRsASense, minLen = 50,
maxN=0, maxEE=c(1,1), truncQ=2, rm.phix=TRUE,
matchIDs=TRUE,compress=TRUE, multithread=7)
filtFsASense <- sort(list.files("../7.DADA2/filtered", pattern="_A_F_filt.fastq.gz", full.names = TRUE))
filtRsASense <- sort(list.files("../7.DADA2/filtered", pattern="_A_R_filt.fastq.gz", full.names = TRUE))
# Now lets learn the error rates
errF <- learnErrors(filtFsASense, multithread=7,
errorEstimationFunction = loessErrfun_mod4,
verbose = TRUE)
errR <- learnErrors(filtRsASense, multithread=7,
errorEstimationFunction = loessErrfun_mod4,
verbose = TRUE)
#lets visualise the error rates
pdf("../7.DADA2/A_EUK_errorF.pdf")
plotErrors(errF)
dev.off()
pdf("../7.DADA2/A_EUK_errorR.pdf")
plotErrors(errR, nominalQ=TRUE)
dev.off()
# Now lets dereplicate the samples, some of our files have dissapeared so we use a complex expression to restore them
derepFs <- derepFastq(list.files("../7.DADA2/filtered", pattern="_A_F_filt.fastq.gz", full.names = TRUE), verbose=TRUE)
derepRs <- derepFastq(list.files("../7.DADA2/filtered", pattern="_A_R_filt.fastq.gz", full.names = TRUE), verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sapply(strsplit(basename(list.files("../7.DADA2/filtered", pattern="_A_F_filt.fastq.gz", full.names = TRUE)), "_"), `[`, 1)
names(derepRs) <- sapply(strsplit(basename(list.files("../7.DADA2/filtered", pattern="_A_R_filt.fastq.gz", full.names = TRUE)), "_"), `[`, 1)
# Now we can pply the DADA2 algorithm to the data
dadaFsASense <- dada(derepFs, err=errF, multithread=TRUE,pool=TRUE)
dadaRsASense <- dada(derepRs, err=errR, multithread=TRUE,pool=TRUE)
# We can now merge the seqs
mergers <- mergePairs(dadaFsASense, derepFs, dadaRsASense, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Now we can construct a sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=7, verbose=TRUE)
dim(seqtab.nochim)
#Lets count up the reads for the sense section of the pipeline
#How many reads do we lose?
getN <- function(x) sum(getUniques(x))
track <- cbind(sapply(dadaFsSense, getN), sapply(dadaRsSense, getN), sapply(mergers, getN),rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- paste0("S.",c("denoisedF", "denoisedR", "merged", "lengthtrunc", "nonchim"))
rownames(track) <- names(dadaFsSense)
head(track)
#Let's save the Sense dataset and rerun the pipeline for the Antisense
EUK.ASense <- seqtab.nochim
EUK.ASenseTrackedReads <- track
##Cheeky function from Ben, see https://github.com/benjjneb/dada2/issues/132#issuecomment-255050128
sumSequenceTables <- function(table1, table2, ..., orderBy = "abundance") {
# Combine passed tables into a list
tables <- list(table1, table2)
tables <- c(tables, list(...))
# Validate tables
if(!(all(sapply(tables, dada2:::is.sequence.table)))) {
stop("At least two valid sequence tables, and no invalid objects, are expected.")
}
sample.names <- rownames(tables[[1]])
for(i in seq(2, length(tables))) {
sample.names <- c(sample.names, rownames(tables[[i]]))
}
seqs <- unique(c(sapply(tables, colnames), recursive=TRUE))
sams <- unique(sample.names)
# Make merged table
rval <- matrix(0L, nrow=length(sams), ncol=length(seqs))
rownames(rval) <- sams
colnames(rval) <- seqs
for(tab in tables) {
rval[rownames(tab), colnames(tab)] <- rval[rownames(tab), colnames(tab)] + tab
}
# Order columns
if(!is.null(orderBy)) {
if(orderBy == "abundance") {
rval <- rval[,order(colSums(rval), decreasing=TRUE),drop=FALSE]
} else if(orderBy == "nsamples") {
rval <- rval[,order(colSums(rval>0), decreasing=TRUE),drop=FALSE]
}
}
rval
}
#Merge Tables
RevComp <- function(input){
require(Biostrings)
dna <- DNAString(input)
dna <- reverseComplement(dna)
return(toString(dna))}
dim(EUK.Sense)
dim(EUK.ASense)
EUK.ASense.RC <- EUK.ASense
colnames(EUK.ASense.RC) <- sapply(colnames(EUK.ASense),FUN=RevComp)
EUK.TotalTable <- sumSequenceTables(EUK.Sense,EUK.ASense.RC)
dim(EUK.TotalTable)
write.csv(EUK.TotalTable,"../6.mappings/OTUtabs/EUK.raw.csv")
#Output OTUs for taxonomy
ASVs <- DNAStringSet(getSequences(EUK.TotalTable))
names(ASVs) <- paste0("ASV_",1:length(ASVs))
writeXStringSet(ASVs,"../5.OTUs/EUK.DADA2.ASVs.fasta", append=FALSE,
compress=FALSE, compression_level=NA, format="fasta")
#Save output with OTU names
EUK.TotalTable.named <- EUK.TotalTable
colnames(EUK.TotalTable.named) <-paste0("ASV_",1:length(ASVs))
write.table(as.data.frame(t(EUK.TotalTable.named)),"../6.mappings/OTUtabs/EUK.raw.names.csv",sep=",")
PR2assign <- assignTaxonomy(getSequences(EUK.TotalTable),refFasta = "../pr2_version_5.0.0_SSU_dada2.fasta.gz",tryRC = TRUE,multithread = TRUE,taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"))
SILVAassign <- assignTaxonomy(getSequences(EUK.TotalTable),refFasta = "../silva_nr99_v138.1_train_set.fa.gz",tryRC = TRUE,multithread = TRUE)
View(PR2assign)
View(PR2assign)
View(SILVAassign)
View(PR2assign)
SILVAassign <- assignTaxonomy(getSequences(EUK.TotalTable),refFasta = "../silva_nr99_v138.1_train_set.fa.gz",tryRC = TRUE,multithread = TRUE,outputBootstraps = TRUE)
View(SILVAassign)
PR2assign <- assignTaxonomy(getSequences(EUK.TotalTable),refFasta = "../pr2_version_5.0.0_SSU_dada2.fasta.gz",tryRC = TRUE,multithread = TRUE,taxLevels = c("Kingdom","Supergroup","Division","Class","Order","Family","Genus","Species"),outputBootstraps = TRUE)
master <- cbind(SILVAassign$tax,SILVAassign$boot)
View(master)
master <- cbind(SILVAassign$tax,SILVAassign$boot,PR2assign$tax,PR2assign$boot)
setwd("/Volumes/BackUp02/Ext.HT.Sequence.data/2023/SC.Sakgerrak.MTB/")
write.csv(master,"mastertax.csv")
View(EUK.TotalTable.named)
PR2master <- cbind(colnames(EUK.TotalTable.named),PR2assign$tax,PR2assign$boot)
SILVAmaster <- cbind(colnames(EUK.TotalTable.named),SILVAassign$tax,SILVAassign$boot)
write.csv(PR2master,"tax.PR2.csv")
write.csv(SILVAmaster,"tax.SILVA.csv")
euk.tax <- ParseTaxonomy(pctThreshold = 99,
covpct = 95,
blastoutput = "taxonomy/raw/EUK.dada2.raw.taxonomy.txt",
lineages = "taxonomy/ncbi_lineages_2023-04-27.csv.gz")
getwd("")
getwd()
setwd("~/GitHubRepos/SC.SkagerrakMTB")
euk.tax <- ParseTaxonomy(pctThreshold = 99,
covpct = 95,
blastoutput = "taxonomy/raw/  ",
lineages = "taxonomy/ncbi_lineages_2023-04-27.csv.gz")
euk.tax <- ParseTaxonomy(pctThreshold = 99,
covpct = 95,
blastoutput = "taxonomy/raw/EUK.dada2.raw.taxonomy.txt",
lineages = "taxonomy/ncbi_lineages_2023-04-27.csv.gz")
write.csv(euk.tax,"taxonomy/EUK.parsed.csv")
####====0.0 Packages & Parameters====####
library("Biostrings")
#Set some variables
minreads <- 2
items <- NULL
#Set the seed
set.seed("123456")
#Read in metadata
metadata<-read.csv("metadata.csv")
metadata$rep <- gsub(".*-([0-9])$","\\1",metadata$SampleID)
####====1.0 Make a raw dataset to play with/visualise====####
EUK.data<- read.csv("rawData/EUK.raw.names.csv",row.names = 1)
EUK.tax <- read.csv("taxonomy/EUK.parsed.csv",row.names = 1)
colnames(EUK.data)
EUK.all <- cbind(EUK.data,EUK.tax[match(rownames(EUK.data),EUK.tax$OTU),])
sampleIndex <- gsub("(^.*)[.][0-9]$","\\1",colnames(EUK.data))
unique(sampleIndex)
EUK.nReps <- data.frame(matrix(0,nrow = length(EUK.data[,1]),ncol=length(unique(sampleIndex))))
colnames(EUK.nReps) <- unique(sampleIndex)
rownames(EUK.nReps) <- rownames(EUK.data)
EUK.binary <-EUK.data
EUK.binary[EUK.binary<1] <- 0
EUK.binary[EUK.binary>0] <- 1
binaryIndex <- gsub("(^.*)[.][0-9]$","\\1",colnames(EUK.binary))
for (column in 1:length(EUK.nReps[1,])){
EUK.nReps[,column] <- rowSums(EUK.binary[,binaryIndex %in% colnames(EUK.nReps)[column]])
}
EUK.all.nReps <- cbind(EUK.nReps,EUK.tax[match(rownames(EUK.binary),EUK.tax$OTU),])
write.csv(EUK.all,"rawdata/EUK.raw.wTAX.csv")
write.csv(EUK.all.nReps,"rawdata/EUK.raw.nReps.wTAX.csv")
RIZ.data<- read.csv("rawData/RIZ.raw.names.csv",row.names = 1)
RIZ.tax <- read.csv("taxonomy/RIZ.parsed.csv",row.names = 1)
colnames(RIZ.data)
RIZ.all <- cbind(RIZ.data,RIZ.tax[match(rownames(RIZ.data),RIZ.tax$OTU),])
sampleIndex <- gsub("(^.*)[.][0-9]$","\\1",colnames(RIZ.data))
unique(sampleIndex)
RIZ.nReps <- data.frame(matrix(0,nrow = length(RIZ.data[,1]),ncol=length(unique(sampleIndex))))
colnames(RIZ.nReps) <- unique(sampleIndex)
rownames(RIZ.nReps) <- rownames(RIZ.data)
RIZ.binary <-RIZ.data
RIZ.binary[RIZ.binary<1] <- 0
RIZ.binary[RIZ.binary>0] <- 1
binaryIndex <- gsub("(^.*)[.][0-9]$","\\1",colnames(RIZ.binary))
for (column in 1:length(RIZ.nReps[1,])){
if (is.vector(RIZ.binary[,binaryIndex %in% colnames(RIZ.nReps)[column]])){
RIZ.nReps[,column]  <- RIZ.binary[,binaryIndex %in% colnames(RIZ.nReps)[column]]}else{
RIZ.nReps[,column] <- rowSums(RIZ.binary[,binaryIndex %in% colnames(RIZ.nReps)[column]])}
}
RIZ.all.nReps <- cbind(RIZ.nReps,RIZ.tax[match(rownames(RIZ.binary),RIZ.tax$OTU),])
write.csv(RIZ.all,"rawdata/RIZ.raw.wTAX.csv")
write.csv(RIZ.all.nReps,"rawdata/RIZ.raw.nReps.wTAX.csv")
####====2.0 Now let's try and do some filtering and make some clean datasets====####
# first lets write this little function to help us collapse replicates
NrepsMaker <- function(INdataframe,vector){
##write these checks
#check the dataframe is a dataframe
if(!is.data.frame(INdataframe)){stop("Input dataframe doesn't look like a dataframe")}
#check the vector is a vector
if(!is.vector(vector)){stop("Input vector doesn't look like a vector")}
#check the dataframe contains the vector
## TO DO
#make a new dataframe to captuire the output
newDataFrame <- data.frame(matrix(0,nrow = length(INdataframe[,1]),ncol=length(unique(vector))))
#name stuff
colnames(newDataFrame) <- unique(vector)
rownames(newDataFrame) <- rownames(INdataframe)
#make it binary
INdataframe[INdataframe<1] <- 0
INdataframe[INdataframe>0] <- 1
#loop over all the samples with replicates, summing according to the vector
for (column in 1:length(newDataFrame[1,])){
# this if statement checks in case there is only one replicate remaining (this sometimes happens in controls)
if(is.vector(INdataframe[,vector %in% colnames(newDataFrame)[column]])){
newDataFrame[,column]  <- INdataframe[,vector %in% colnames(newDataFrame)[column]]}else{
newDataFrame[,column] <- rowSums(INdataframe[,vector %in% colnames(newDataFrame)[column]])}
}
return(newDataFrame)
}
# we cut the metadata here becuase it is strangely formatted
metadata <- metadata[0:120,]
list.files("rawdata",pattern="....raw.names.csv")
list.files("rawdata",pattern="....raw.names.csv")[1]
dataset <- list.files("rawdata",pattern="....raw.names.csv")[1]
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
View(expSamples)
View(EUK.data)
View(EUK.binary)
View(EUK.nReps)
EUK.asvs <- read.fasta
EUK.asvs <- read.fasta
EUK.asvs <-
?read.fasta
EUK.asvs <-
?read.fasta
?read.fasta
??read.fasta
??readFasta
library(seqinr)
EUK.asvs <- readFasta("rawdata/ASVs/EUK.DADA2.ASVs.fasta")
EUK.asvs <- read.fasta("rawdata/ASVs/EUK.DADA2.ASVs.fasta")
View(EUK.asvs)
EUK.asvs <- read.fasta("rawdata/ASVs/EUK.DADA2.ASVs.fasta",as.string = TRUE)
View(EUK.asvs)
EKA.asvs[1]
EUK.asvs[1]
lapply(EUK.asvs,length())
test <- unlist(EUK.asvs)
test
names(test)
length("ASSB")
n_char("ASSB")
nchar("ASSB")
nchar(unlist(EUK.asvs))
EUK.asv.len <- nchar(unlist(EUK.asvs))
hist(EUK.asv.len)
hist(EUK.asv.len,breaks=100)
EUK.asv.len>75*EUK.asv.len<150
EUK.asv.len>75 & EUK.asv.len<150
EUK.asv.len<75 | EUK.asv.len>150
table(EUK.asv.len<75 | EUK.asv.len>150)
View(EUK.all)
write.csv(EUK.all[EUK.asv.len<75 | EUK.asv.len>150,],"rawdata/EUK.raw.wTAXDIRTY.csv")
write.csv(EUK.all[EUK.asv.len<100 | EUK.asv.len>150,],"rawdata/EUK.raw.wTAXDIRTY.csv")
datasetname <- substr(dataset,1,3)
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
nchar(rawSeqs)
nchar(rawSeqs)>75
test <- CleanedOutput[nchar(rawSeqs)<75 | nchar(rawSeqs)>150,]
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
controlsCONTAM <- ctlSamples[rowSums(ctlSamples) > 0,]
for (contamOTU in 1:length(controlsCONTAM[,1])){
loopOTU <- row.names(controlsCONTAM[contamOTU,])
loopMax <- max(as.numeric(controlsCONTAM[contamOTU,]))
#loopSum <- sum(as.numeric(controlsCONTAM[contamOTU,]))
if (any(is.na(expSamples[loopOTU,]))){next}
expSamples[loopOTU,expSamples[loopOTU,]<loopMax] <- 0
print(paste("Cleaning contaminants",contamOTU))
}
##### make a version of the data with Nreps
expSamplesNreps <- NrepsMaker(expSamples,gsub("(^.*)[.][0-9]$","\\1",colnames(expSamples)))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
Assignments <- read.csv(paste0("taxonomy/",datasetname,".parsed.csv"),row.names = 1)
CleanedOutput <- cbind(expSamples,
unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))],
Assignments[match(row.names(expSamples),Assignments$OTU),])
dir.create("cleanedData",showWarnings = F)
test <- CleanedOutput[nchar(rawSeqs)<75 | nchar(rawSeqs)>150,]
View(test)
test <- CleanedOutput[nchar(rawSeqs)>75 | nchar(rawSeqs)<150,]
test <- CleanedOutput[nchar(rawSeqs)>75 | nchar(rawSeqs)<150,]
View(test)
test <- CleanedOutput[nchar(rawSeqs)>75 | nchar(rawSeqs)<150,]
View(CleanedOutput)
View(CleanedOutput)
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
controlsCONTAM <- ctlSamples[rowSums(ctlSamples) > 0,]
for (contamOTU in 1:length(controlsCONTAM[,1])){
loopOTU <- row.names(controlsCONTAM[contamOTU,])
loopMax <- max(as.numeric(controlsCONTAM[contamOTU,]))
#loopSum <- sum(as.numeric(controlsCONTAM[contamOTU,]))
if (any(is.na(expSamples[loopOTU,]))){next}
expSamples[loopOTU,expSamples[loopOTU,]<loopMax] <- 0
print(paste("Cleaning contaminants",contamOTU))
}
##### make a version of the data with Nreps
expSamplesNreps <- NrepsMaker(expSamples,gsub("(^.*)[.][0-9]$","\\1",colnames(expSamples)))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
Assignments <- read.csv(paste0("taxonomy/",datasetname,".parsed.csv"),row.names = 1)
CleanedOutput <- cbind(expSamples,
unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))],
Assignments[match(row.names(expSamples),Assignments$OTU),])
dir.create("cleanedData",showWarnings = F)
test <- CleanedOutput[nchar(rawSeqs)>75 | nchar(rawSeqs)<150,]
View(test)
View(CleanedOutput)
View(CleanedOutput)
nchar(rawSeqs)>75
nchar(rawSeqs)>75 | nchar(rawSeqs)<150
table(nchar(rawSeqs)>75 | nchar(rawSeqs)<150)
test <- CleanedOutput[nchar(rawSeqs)>75 & nchar(rawSeqs)<150,]
cleanSeqs <- unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))]
test <- CleanedOutput[nchar(cleanSeqs)>75 & nchar(cleanSeqs)<150,]
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
Assignments <- read.csv(paste0("taxonomy/",datasetname,".parsed.csv"),row.names = 1)
CleanedOutput <- cbind(expSamples,
unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))],
Assignments[match(row.names(expSamples),Assignments$OTU),])
dir.create("cleanedData",showWarnings = F)
cleanSeqs <- unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))]
test <- CleanedOutput[nchar(cleanSeqs)>75 & nchar(cleanSeqs)<150,]
write.csv(CleanedOutput,paste0("cleanedData/clean.",dataset,".csv"))
(datasetname == "EUK")
size <- rbind("EUK"=c(75,150),"RIZ"=c(75,140))
View(size)
dataset <- list.files("rawdata",pattern="....raw.names.csv"){2}
dataset <- list.files("rawdata",pattern="....raw.names.csv")[2]
dataset
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
controlsCONTAM <- ctlSamples[rowSums(ctlSamples) > 0,]
for (contamOTU in 1:length(controlsCONTAM[,1])){
loopOTU <- row.names(controlsCONTAM[contamOTU,])
loopMax <- max(as.numeric(controlsCONTAM[contamOTU,]))
#loopSum <- sum(as.numeric(controlsCONTAM[contamOTU,]))
if (any(is.na(expSamples[loopOTU,]))){next}
expSamples[loopOTU,expSamples[loopOTU,]<loopMax] <- 0
print(paste("Cleaning contaminants",contamOTU))
}
size <- rbind("EUK"=c(75,150),"RIZ"=c(75,140))
##### make a version of the data with Nreps
expSamplesNreps <- NrepsMaker(expSamples,gsub("(^.*)[.][0-9]$","\\1",colnames(expSamples)))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
Assignments <- read.csv(paste0("taxonomy/",datasetname,".parsed.csv"),row.names = 1)
CleanedOutput <- cbind(expSamples,
unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))],
Assignments[match(row.names(expSamples),Assignments$OTU),])
dir.create("cleanedData",showWarnings = F)
loopSeqs <-unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))]
hist(nchar(loopSeqs))
hist(nchar(loopSeqs),breaks=100)
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
nchar(rawSeqs)
datasetname
size[datasetname]
size[datasetname,]
size[datasetname,2]
nchar(rawSeqs)<size[datasetname,2]
nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2]
table(nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2])
datasetname <- "EUK"
size <- rbind("EUK"=c(75,150),"RIZ"=c(75,140))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2]
table(nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2])
greenlistLength <- names(nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2])
greenlistLength
greenlistLength <- names(rawSeqs[nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2]])
names(expSamples)
colnames(expSamples)
rownames(expSamples)
rownames(expSamples) %in% greenlistLength
dataset <- list.files("rawdata",pattern="....raw.names.csv")[1]
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
controlsCONTAM <- ctlSamples[rowSums(ctlSamples) > 0,]
for (contamOTU in 1:length(controlsCONTAM[,1])){
loopOTU <- row.names(controlsCONTAM[contamOTU,])
loopMax <- max(as.numeric(controlsCONTAM[contamOTU,]))
#loopSum <- sum(as.numeric(controlsCONTAM[contamOTU,]))
if (any(is.na(expSamples[loopOTU,]))){next}
expSamples[loopOTU,expSamples[loopOTU,]<loopMax] <- 0
print(paste("Cleaning contaminants",contamOTU))
}
size <- rbind("EUK"=c(75,150),"RIZ"=c(75,140))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
greenlistLength <- names(rawSeqs[nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2]])
rownames(expSamples) %in% greenlistLength
table(rownames(expSamples) %in% greenlistLength)
expSamples <- expSamples[rownames(expSamples) %in% greenlistLength,]
for (dataset in list.files("rawdata",pattern="....raw.names.csv")){
datasetname <- substr(dataset,1,3)
indata <- read.csv(paste0("rawdata/",datasetname,".raw.names.csv"),row.names = 1)
expSamples <- indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="Experimental"])),substring(colnames(indata),5)))]
ctlSamples <-indata[,na.omit(match(gsub(",|-",".",sort(metadata$SampleID[metadata$SampleType=="ControlN"])),substring(colnames(indata),5)))]
#Filter 1 - minimum number of reads for any ID
expSamples[expSamples< minreads] <- 0
expSamples <- expSamples[rowSums(expSamples) > 0,]
#####NOTE - we are not running this filter as we want to be V sensitive!
#Filter 2 - within samples OTU must appear in more than one sample (this works because there are lots of reps per site and sample)
#filtersam <- expSamples
#filtersam[filtersam>0 ] <- 1
#filtersam <-filtersam[rowSums(filtersam) > 1,]
#expSamples <- expSamples[rownames(expSamples) %in% rownames(filtersam),]
#Filter 3 -Maximum value in neg = 0 value in samples
controlsCONTAM <- ctlSamples[rowSums(ctlSamples) > 0,]
for (contamOTU in 1:length(controlsCONTAM[,1])){
loopOTU <- row.names(controlsCONTAM[contamOTU,])
loopMax <- max(as.numeric(controlsCONTAM[contamOTU,]))
#loopSum <- sum(as.numeric(controlsCONTAM[contamOTU,]))
if (any(is.na(expSamples[loopOTU,]))){next}
expSamples[loopOTU,expSamples[loopOTU,]<loopMax] <- 0
print(paste("Cleaning contaminants",contamOTU))
}
##Filter 4 - exclude OTUs/ASVS outside of a size range
size <- rbind("EUK"=c(75,150),"RIZ"=c(75,140))
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
greenlistLength <- names(rawSeqs[nchar(rawSeqs)>size[datasetname,1] & nchar(rawSeqs)<size[datasetname,2]])
expSamples <- expSamples[rownames(expSamples) %in% greenlistLength,]
##### make a version of the data with Nreps
expSamplesNreps <- NrepsMaker(expSamples,gsub("(^.*)[.][0-9]$","\\1",colnames(expSamples)))
#Reattach taxonomy and ASVs
rawSeqs <- as.character(readDNAStringSet(paste0("rawdata/ASVs/",datasetname,".DADA2.ASVs.fasta")))
Assignments <- read.csv(paste0("taxonomy/",datasetname,".parsed.csv"),row.names = 1)
CleanedOutput <- cbind(expSamples,
unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))],
Assignments[match(row.names(expSamples),Assignments$OTU),])
dir.create("cleanedData",showWarnings = F)
cleanSeqs <- unname(rawSeqs)[match(row.names(expSamples),names(rawSeqs))]
write.csv(CleanedOutput,paste0("cleanedData/clean.",dataset,".csv"))
CleanedNrepsOutput <- cbind(expSamplesNreps,
unname(rawSeqs)[match(row.names(expSamplesNreps),names(rawSeqs))],
Assignments[match(row.names(expSamplesNreps),Assignments$OTU),])
write.csv(CleanedNrepsOutput,paste0("cleanedData/clean.",dataset,".Nreps.csv"))
}
